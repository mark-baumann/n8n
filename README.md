# LangGraph RAG Multiâ€‘Agent Chatbot (von Basics â†’ Advanced)

Ein komplettes, lokal lauffÃ¤higes Projekt, das Schritt fÃ¼r Schritt von einem einfachen LLMâ€‘Call
zu einem **zustandsbehafteten** (checkpointed) **Multiâ€‘Agent**â€‘Chatbot mit **RAG** (Qdrant Cloud oder FAISS) und optionaler **Websuche**
ausgebaut wird â€“ basierend auf **LangGraph** + **LangChain**.

## Features
- âœ… **RAG** via Qdrant Cloud (Standard) oder lokalem FAISS-Fallback (`VECTORSTORE_BACKEND`)
- âœ… **Multiâ€‘Agent Routing** (`direct` | `rag` | `web`) via strukturierter LLMâ€‘Entscheidung
- âœ… **Toolâ€‘Aufrufe** ohne `prebuilt`-AbhÃ¤ngigkeit (robust gegen APIâ€‘Ã„nderungen)
- âœ… **Memory / Checkpointing** via `MemorySaver` (optional: SQLiteâ€‘Saver)
- âœ… **CLI** (`python -m app.cli`) und **kleine FastAPI** (`uvicorn app.api.server:app --reload`)
- âœ… **Konfigurierbar** via `.env` (OpenAI / HFâ€‘Embeddings, Websuche an/aus, Modelle)

---

## 1) Voraussetzungen
- Python **3.11**+ empfohlen
- `pip` bzw. `uv`

## 2) Setup
```bash
git clone <dieses-Archiv-oder-zip-entpacken>
cd langgraph_rag_multiagent
python -m venv .venv && source .venv/bin/activate   # Windows: .venv\Scripts\activate
pip install -r requirements.txt
cp .env.example .env
# In .env API-Keys eintragen (mind. OPENAI_API_KEY fÃ¼r LLM + Embeddings, falls OpenAI gewÃ¤hlt)
```

### Docker-Variante

Alternativ kannst Du das komplette Backend inklusive WeboberflÃ¤che per Docker starten:

```bash
cp .env.example .env  # gewÃ¼nschte Keys/Settings eintragen
docker compose up --build
```

Der FastAPI-Server lÃ¤uft anschlieÃŸend unter http://127.0.0.1:8000 (Frontend ist im selben Container eingebettet).
Dokumente und Artefakte werden aus dem lokalen `data/`- bzw. `artifacts/`-Ordner in den Container gemountet, sodass
Du sie bequem bearbeiten kannst.

> **Hinweis:** StandardmÃ¤ÃŸig nutzt das Projekt OpenAIâ€‘Modelle. Alternativ kannst Du **HuggingFace**â€‘Embeddings wÃ¤hlen,
> dann brauchst Du keinen OpenAIâ€‘Key fÃ¼r die Vektordatenbank (nur fÃ¼rs LLM, sofern Du ein OpenAIâ€‘LLM nutzt).

## 3) Eigene Dokumente indizieren (RAG)
Lege Deine Dateien unter `data/docs/` ab (unterstÃ¼tzt: `.md`, `.txt`, `.pdf`). Danach:
```bash
python -m app.vectorstore.ingest
```
StandardmÃ¤ÃŸig wird dabei eine Qdrantâ€‘Collection aufgebaut (`VECTORSTORE_BACKEND=qdrant`).
Hinterlege dazu in `.env` Deine Qdrantâ€‘Cloudâ€‘URL inkl. Port (`:6333`), den APIâ€‘Key und
optional einen Collectionâ€‘Namen (`QDRANT_COLLECTION`).

> **Tipp (Docker):** `docker compose run --rm app python -m app.vectorstore.ingest`

Wenn Du lieber lokal arbeiten mÃ¶chtest, setze `VECTORSTORE_BACKEND=faiss`. Dann wird ein
persistenter Index unter `data/index/faiss/` abgelegt. Falls der direkte Aufruf der
OpenAIâ€‘API durch einen Proxy blockiert ist, wechsle per `EMBEDDINGS_PROVIDER=huggingface`
auf Sentenceâ€‘Transformer.

## 4) Chatten (CLI)
```bash
python -m app.cli --thread demo
```
- `--thread` identifiziert die Konversations-Session (wichtig fÃ¼r Memory/Checkpoints).

## 5) (Optional) API starten
```bash
uvicorn app.api.server:app --reload
# POST http://127.0.0.1:8000/chat  JSON: {"thread_id":"demo", "message":"<Deine Frage>"}
```

### WeboberflÃ¤che nutzen
- Ã–ffne im Browser: http://127.0.0.1:8000/
- WÃ¤hle eine Thread-ID (z.â€¯B. `demo`) oder erzeuge eine neue.
- Stelle Deine Fragen â€“ Antworten erscheinen gemeinsam mit dem Chatverlauf.

## 6) Routen & Agents
- **Router** (LLM mit strukturiertem Output) entscheidet: `direct` (direkt antworten), `rag` (Vektorâ€‘Suche) oder `web` (Websuche).
- **RAGâ€‘Agent**: nutzt den `retrieve`â€‘Tool (Qdrant oder FAISS) iterativ, bis genug Kontext vorhanden ist, dann Antwort mit Quellen.
- **Webâ€‘Agent**: nutzt `web_search` (DuckDuckGo ohne Key oder Tavily â€“ wenn Key gesetzt).

## 7) Visualisierung (optional)
Wenn `graphviz` installiert ist, kannst Du den Graph als PNG exportieren:
```bash
python -m app.graph_render
```
Bild wird unter `artifacts/graph.png` abgelegt.

## 8) Testschnelllauf
```bash
# (1) Index bauen (mit den mitgelieferten Beispieldateien)
python -m app.vectorstore.ingest

# (2) Kurzer CLI-Test
python -m app.cli --thread quick --message "Worum geht es in den Beispieldokumenten?"

# (3) Websuche aktivieren (optional)
# In .env: ENABLE_WEBSEARCH=true
python -m app.cli --thread quick --message "Was ist neu in LangGraph im Jahr 2025?"
```

---

## Projektstruktur
```
app/
  api/server.py            # Kleine FastAPI fÃ¼r HTTP-Chat
  agents/
    tools.py               # Retriever-Tool + Websuche-Tool
    router.py              # LLM-Router (structured output)
  graph.py                 # Bau des Multi-Agent Graphen
  graph_render.py          # PNG-Export des Graphen (optional)
  models/llm.py            # LLM-Initialisierung (OpenAI, streaming-ready)
  prompts/                 # Systemprompts
  schemas.py               # State (TypedDict) + Reducer
  vectorstore/
    ingest.py              # Index erstellen
    retriever.py           # Index laden â†’ Retriever
  cli.py                   # CLI-Einstieg
data/
  docs/                    # Deine Dokumente fÃ¼r RAG
  index/                   # Persistenter FAISS-Index (nur bei VECTORSTORE_BACKEND=faiss)
artifacts/                 # (optional) Graph-Bild, Dumps
.env.example               # Konfigurationsbeispiel
requirements.txt
README.md
```

---

## Hinweise
- StandardmÃ¤ÃŸig **MemorySaver** (inâ€‘memory) als Checkpointer. FÃ¼r Persistenz:
  setze `CHECKPOINTER_BACKEND=sqlite` in `.env` (benÃ¶tigt `langgraph-checkpoint-sqlite`).
- Websuche ohne Key nutzt `duckduckgo_search`. FÃ¼r **Tavily** setze `TAVILY_API_KEY` und `WEBSEARCH_BACKEND=tavily`.

Viel SpaÃŸ! ðŸš€
