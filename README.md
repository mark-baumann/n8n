# LangGraph RAG Multiâ€‘Agent Chatbot (von Basics â†’ Advanced)

Ein komplettes, lokal lauffÃ¤higes Projekt, das Schritt fÃ¼r Schritt von einem einfachen LLMâ€‘Call
zu einem **zustandsbehafteten** (checkpointed) **Multiâ€‘Agent**â€‘Chatbot mit **RAG** (lokaler FAISSâ€‘Index) und optionaler **Websuche**
ausgebaut wird â€“ basierend auf **LangGraph** + **LangChain**.

## Features
- âœ… **RAG** via lokalem FAISS-Index (Voreinstellung, offline-freundlich)
- âœ… **Multiâ€‘Agent Routing** (`direct` | `rag` | `web`) via strukturierter LLMâ€‘Entscheidung
- âœ… **Toolâ€‘Aufrufe** ohne `prebuilt`-AbhÃ¤ngigkeit (robust gegen APIâ€‘Ã„nderungen)
- âœ… **Memory / Checkpointing** via `MemorySaver` (optional: SQLiteâ€‘Saver)
- âœ… **CLI** (`python -m app.cli`) und **kleine FastAPI** (`uvicorn app.api.server:app --reload`)
- âœ… **Browser-OberflÃ¤che** mit fokussiertem Upload, Vorschau & Chat in einem Screen
- âœ… **Konfigurierbar** via `.env` (OpenAI / HFâ€‘Embeddings, Websuche an/aus, Modelle)

---

## 1) Voraussetzungen
- Python **3.11**+ empfohlen
- `pip` bzw. `uv`

## 2) Setup
```bash
git clone <dieses-Archiv-oder-zip-entpacken>
cd langgraph_rag_multiagent
python -m venv .venv && source .venv/bin/activate   # Windows: .venv\Scripts\activate
pip install -r requirements.txt
cp .env.example .env
# In .env API-Keys eintragen (z.â€¯B. OPENAI_API_KEY fÃ¼r das LLM oder Embeddings, falls OpenAI gewÃ¤hlt)
```

### Docker-Variante

Alternativ kannst Du das komplette Backend inklusive WeboberflÃ¤che per Docker starten:

```bash
cp .env.example .env  # gewÃ¼nschte Keys/Settings eintragen
docker compose up --build
```

Der FastAPI-Server lÃ¤uft anschlieÃŸend unter http://127.0.0.1:8000 (Frontend ist im selben Container eingebettet).
Dokumente und Artefakte werden aus dem lokalen `data/`- bzw. `artifacts/`-Ordner in den Container gemountet, sodass
Du sie bequem bearbeiten kannst.

> **Hinweis:** StandardmÃ¤ÃŸig nutzt der Index `text-embedding-3-small` von OpenAI â€“ trage dazu Deinen `OPENAI_API_KEY` in `.env` ein.
> Sollte der Aufruf scheitern (z.â€¯B. wegen Proxy), fÃ¤llt das System automatisch auf den lokalen Hashing-Embedder (`local-hash-768`) zurÃ¼ck.

## 3) Eigene Dokumente indizieren (RAG)
Die WeboberflÃ¤che enthÃ¤lt einen Upload-Dialog. Jedes hochgeladene PDF/Markdown/Text-Dokument wird
automatisch nach `data/docs/` gespeichert und der FAISS-Index unmittelbar aktualisiert â€“ Du musst
also kein separates Kommando ausfÃ¼hren und kannst direkt danach chatten.

> ðŸ“‚ Das Repository liefert absichtlich **keine Beispiel-Dokumente** mit. Lade Deine eigenen Dateien
> hoch oder lege sie manuell in `data/docs/` ab.

> ðŸ’¡ Falls Du Dokumente auÃŸerhalb der WeboberflÃ¤che in `data/docs/` ablegst, kannst Du den Index bei
> Bedarf weiterhin manuell per `python -m app.vectorstore.ingest` erneuern. FÃ¼r den normalen Upload-
> Workflow ist dieser Schritt jedoch nicht nÃ¶tig.

Der Index wird unter `data/index/faiss/` abgelegt. Wenn OpenAI als Embedding-Provider konfiguriert ist,
fÃ¤llt die Indizierung bei Erreichbarkeitsproblemen automatisch auf den Hashing-Embedder zurÃ¼ck.

## 4) Chatten (CLI)
```bash
python -m app.cli --thread demo
```
- `--thread` identifiziert die Konversations-Session (wichtig fÃ¼r Memory/Checkpoints).

## 5) (Optional) API starten
```bash
uvicorn app.api.server:app --reload
# POST http://127.0.0.1:8000/chat  JSON: {"thread_id":"demo", "message":"<Deine Frage>"}
```

### WeboberflÃ¤che nutzen
- Ã–ffne im Browser: http://127.0.0.1:8000/
- Lade Dein Dokument Ã¼ber den Upload-Button oben rechts.
- Die Vorschau Ã¶ffnet PDFs direkt im integrierten Browser-Viewer (andere Dateien als Textauszug).
- Stelle Deine Fragen in der Chat-Leiste am unteren Rand â€“ der Assistent antwortet mit Kontext aus dem aktuell angezeigten Dokument.

## 6) Routen & Agents
- **Router** (LLM mit strukturiertem Output) entscheidet: `direct` (direkt antworten), `rag` (Vektorâ€‘Suche) oder `web` (Websuche).
- **RAGâ€‘Agent**: nutzt den `retrieve`â€‘Tool (lokaler FAISS-Index) iterativ, bis genug Kontext vorhanden ist, dann Antwort mit Quellen.
- **Webâ€‘Agent**: nutzt `web_search` (DuckDuckGo ohne Key oder Tavily â€“ wenn Key gesetzt).

## 7) Visualisierung (optional)
Wenn `graphviz` installiert ist, kannst Du den Graph als PNG exportieren:
```bash
python -m app.graph_render
```
Bild wird unter `artifacts/graph.png` abgelegt.

## 8) Testschnelllauf
```bash
# (1) Index bauen (falls bereits Dateien in data/docs/ liegen)
python -m app.vectorstore.ingest

# (2) Kurzer CLI-Test
python -m app.cli --thread quick --message "Worum geht es in den Beispieldokumenten?"

# (3) Websuche aktivieren (optional)
# In .env: ENABLE_WEBSEARCH=true
python -m app.cli --thread quick --message "Was ist neu in LangGraph im Jahr 2025?"
```

---

## Projektstruktur
```
app/
  api/server.py            # Kleine FastAPI fÃ¼r HTTP-Chat
  agents/
    tools.py               # Retriever-Tool + Websuche-Tool
    router.py              # LLM-Router (structured output)
  graph.py                 # Bau des Multi-Agent Graphen
  graph_render.py          # PNG-Export des Graphen (optional)
  models/llm.py            # LLM-Initialisierung (OpenAI, streaming-ready)
  prompts/                 # Systemprompts
  schemas.py               # State (TypedDict) + Reducer
  vectorstore/
    ingest.py              # Index erstellen
    retriever.py           # Index laden â†’ Retriever
  cli.py                   # CLI-Einstieg
data/
  docs/                    # Deine Dokumente fÃ¼r RAG
  index/                   # Persistenter FAISS-Index
artifacts/                 # (optional) Graph-Bild, Dumps
.env.example               # Konfigurationsbeispiel
requirements.txt
README.md
```

---

## Hinweise
- StandardmÃ¤ÃŸig **MemorySaver** (inâ€‘memory) als Checkpointer. FÃ¼r Persistenz:
  setze `CHECKPOINTER_BACKEND=sqlite` in `.env` (benÃ¶tigt `langgraph-checkpoint-sqlite`).
- Websuche ohne Key nutzt `duckduckgo_search`. FÃ¼r **Tavily** setze `TAVILY_API_KEY` und `WEBSEARCH_BACKEND=tavily`.

Viel SpaÃŸ! ðŸš€
